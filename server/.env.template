PORT=5000
CLIENT_URL=http://localhost:5173

# AI Provider Configuration
GEMINI_API_KEY=your_gemini_api_key_here
DEFAULT_AI_PROVIDER=gemini

# Optional AI Providers (leave empty if not using)
OPENAI_API_KEY=
TOGETHER_API_KEY=
GROQ_API_KEY=
HUGGINGFACE_API_KEY=

# Pinecone Configuration (optional - system works without it)
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=
PINECONE_INDEX_NAME=


# RAG toggle (enable LangChain-based Retrieval-Augmented Generation)
RAG_ENABLED=true

# RAG index configuration (defaults are fine for lessons)
RAG_INDEX_NAME=spacey-lessons # falls back to PINECONE_INDEX_NAME if unset
RAG_NAMESPACE=lessons         # namespace for lesson chunks
RAG_TOP_K=6                   # number of chunks to retrieve

# Embeddings provider for RAG
# - openai: uses OpenAI embeddings (requires OPENAI_API_KEY)
# - local: uses on-device Transformers (Xenova) â€” no external key
RAG_EMBED_PROVIDER=local
RAG_EMBED_MODEL=Xenova/bge-large-en-v1.5   # or Xenova/bge-large-en-v1.5 when provider=local

# Chat model used by the RAG chain
# Note: current RAG chain uses an OpenAI chat model by default.
# We can switch to Gemini in a follow-up if preferred.
RAG_MODEL=gemini-2.0-flash-001

CONVERSATIONS_INDEX_NAME=conversations-v1
CONVERSATIONS_NAMESPACE=conversations
CONVERSATIONS_EMBED_MODEL=Xenova/bge-large-en-v1.5
CONVERSATIONS_TOP_K=5

FACTS_LLM_ENABLED=true # to enable the second pass
FACTS_LLM_PROVIDER=gemini # (default) or another configured provider
FACTS_MIN_RULES=1 # (min rule facts to skip LLM)

USER_PROFILE_INDEX_NAME=user-profiles-v1
USER_PROFILE_NAMESPACE=user_profiles

